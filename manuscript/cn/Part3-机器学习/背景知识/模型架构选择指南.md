# 背景知识：深度学习模型架构选择指南

> 选对架构是成功的一半。不同模型适合不同场景，没有"万能模型"。

---

## 一、模型架构速查表

| 模型类型 | 参数规模 | 训练时间 | 推理延迟 | 适用场景 | 优势 | 劣势 |
|---------|---------|---------|---------|---------|------|------|
| **LSTM** | 1-10M | 中等 | <10ms | 短期价格预测、高频交易 | 捕捉时序依赖、训练稳定 | 长序列性能下降 |
| **GRU** | 0.5-5M | 较快 | <8ms | 资源受限场景、实时推理 | 参数少、训练快 | 表达能力略弱于LSTM |
| **Transformer** | 10-100M | 高 | 10-50ms | 多资产组合、长期趋势 | 并行训练、长期依赖 | 数据需求大、过拟合风险 |
| **CNN** | 0.5-5M | 快 | <5ms | 技术图形识别、模式匹配 | 局部特征提取、效率高 | 时序建模能力弱 |
| **CNN-LSTM混合** | 5-20M | 中高 | 10-30ms | 多时间尺度分析 | 结合局部和全局特征 | 复杂度高、调参难 |

---

## 二、LSTM/GRU：时序建模的主力

### 2.1 架构原理

LSTM（长短期记忆网络）通过**三个门控机制**控制信息流动：

```
输入门（Input Gate）：决定哪些新信息写入记忆
遗忘门（Forget Gate）：决定哪些旧信息丢弃
输出门（Output Gate）：决定输出哪些记忆信息
```

**GRU（门控循环单元）**是LSTM的简化版：
- 合并了输入门和遗忘门为"更新门"
- 参数减少约25%，训练速度更快
- 在小数据集上表现与LSTM相当

### 2.2 典型架构配置

```
单标的日频策略：
├── 输入层：20-60个时间步 × 10-30维特征
├── LSTM层1：128单元 + Dropout(0.2)
├── LSTM层2：64单元 + Dropout(0.2)
├── 全连接层：32单元 + ReLU
└── 输出层：1单元（回归）或 3单元（分类：涨/跌/平）

高频交易（分钟级）：
├── 输入层：60-120个时间步 × 50-100维特征
├── GRU层：256单元（速度优先）
├── 全连接层：64单元
└── 输出层：离散动作（买/卖/持有）
```

### 2.3 何时选择LSTM/GRU？

| 场景 | 推荐 | 原因 |
|-----|-----|------|
| 数据量 < 10万样本 | ✅ LSTM/GRU | 小数据集上Transformer容易过拟合 |
| 序列长度 < 100步 | ✅ LSTM/GRU | 短序列LSTM足够，Transformer优势不明显 |
| 推理延迟要求 < 10ms | ✅ GRU | 参数少，推理快 |
| 单标的策略 | ✅ LSTM | 捕捉单一资产的时序模式 |

### 2.4 重要发现

根据arXiv研究"Vanilla LSTMs Outperform Transformer-based Forecasting"：

> 在金融时序预测任务中，标准LSTM在**数据量有限**或**序列长度较短**的场景下，性能常优于更复杂的Transformer架构。

**原因**：金融数据信噪比低，复杂模型容易学习到噪音而非真实规律。

---

## 三、Transformer：长序列与多资产的选择

### 3.1 核心创新

**自注意力机制（Self-Attention）**：
- 能够同时关注序列中所有位置的信息
- 捕捉长距离依赖关系
- 支持并行计算，训练效率高

**位置编码（Positional Encoding）**：
- 保留时间序列的顺序信息
- 弥补注意力机制本身不区分位置的缺陷

### 3.2 金融领域变体

| 变体 | 改进点 | 适用场景 |
|-----|-------|---------|
| **Informer** | 稀疏注意力，降低计算复杂度 | 长序列预测（>1000步）|
| **Autoformer** | 自相关机制，捕捉周期性 | 季节性强的数据 |
| **StockFormer** | 端到端强化学习 | 直接输出交易决策 |
| **Higher-Order Transformer** | 高阶注意力，特征交互 | 股票价格预测（+5-10%准确率）|

### 3.3 何时选择Transformer？

| 场景 | 推荐 | 原因 |
|-----|-----|------|
| 多资产组合（>50只）| ✅ Transformer | 同时建模资产间相互影响 |
| 长序列（>200步）| ✅ Transformer | 长期依赖建模能力强 |
| 数据量 > 100万样本 | ✅ Transformer | 充分发挥模型容量 |
| 宏观经济预测 | ✅ Transformer | 需要捕捉长期趋势 |

### 3.4 注意事项

```
Transformer的陷阱：
1. 过拟合风险高 → 需要强正则化（Dropout≥0.3）
2. 数据需求大 → 样本不足时性能不如LSTM
3. 计算成本高 → GPU训练必需
4. 位置编码敏感 → 需要针对金融数据调整
```

---

## 四、CNN：模式识别的利器

### 4.1 应用方式

**一维CNN**：直接处理价格序列
```
输入：过去60天的OHLCV数据（60×5矩阵）
卷积核：多个尺寸（3,5,7天）提取不同周期特征
池化：最大池化或平均池化
输出：特征向量 → 分类/回归头
```

**二维CNN**：处理K线图图像
```
输入：K线图渲染为图像（如224×224×3）
架构：类似ResNet或VGG
用途：识别头肩顶、双底、三角形等经典形态
```

### 4.2 何时选择CNN？

| 场景 | 推荐 | 原因 |
|-----|-----|------|
| 技术形态识别 | ✅ CNN | 擅长提取局部空间特征 |
| 极低延迟要求 | ✅ CNN | 推理速度最快 |
| 相关性矩阵分析 | ✅ 二维CNN | 可视化多资产关系 |

### 4.3 局限性

```
CNN在金融中的问题：
1. 忽略时序顺序 → 需要配合位置编码或RNN
2. 局部感受野 → 难以捕捉长期依赖
3. K线图主观性 → 不同绘制方式影响结果
```

---

## 五、混合架构：取长补短

### 5.1 CNN-LSTM

```
架构：
输入 → CNN（提取局部特征）→ LSTM（建模时序依赖）→ 输出

优势：
- CNN快速筛选关键特征
- LSTM捕捉时间演化规律
- 多时间尺度融合

劣势：
- 调参复杂度高
- 过拟合风险增加
```

### 5.2 LSTM-Transformer

```
架构：
输入 → LSTM（局部时序）→ Transformer（全局上下文）→ 输出

适用场景：
- 既有短期动量又有长期趋势的市场
- 需要捕捉regime切换的策略
```

### 5.3 混合架构的建议

| 数据特点 | 推荐架构 |
|---------|---------|
| 强短期依赖 + 弱长期依赖 | LSTM主导 |
| 弱短期依赖 + 强长期依赖 | Transformer主导 |
| 两者都重要 | CNN-LSTM 或 LSTM-Transformer |
| 不确定 | 从LSTM开始，逐步增加复杂度 |

---

## 六、强化学习算法选择

### 6.1 核心算法对比

| 算法 | 年化收益率 | 夏普比率 | 最大回撤 | 样本效率 | 训练稳定性 | 适用场景 |
|-----|-----------|---------|---------|---------|-----------|---------|
| **DQN** | 8-15% | 0.6-1.2 | 15-25% | 中等 | 中等（易发散）| 高频交易、离散动作 |
| **PPO** | 15-25% | 1.2-1.8 | 10-18% | 较高 | 高（稳定收敛）| 中低频交易、连续动作 |
| **A3C** | 10-18% | 0.8-1.4 | 12-22% | 较高 | 低（震荡明显）| 并行探索、资源受限 |
| **SAC** | 12-20% | 1.0-1.6 | 12-20% | 较高 | 中高 | 高频交易、鼓励探索 |
| **DDPG** | 8-15% | 0.6-1.2 | 15-25% | 中等 | 低 | 连续动作、精细仓位 |

### 6.2 选择建议

```
从PPO开始 → 它在稳定性和性能间平衡最好

如果需要离散动作（买/卖/持有）→ DQN
如果需要连续动作（仓位比例）→ PPO 或 SAC
如果追求极致探索 → SAC
如果资源充足要并行 → A3C
```

---

## 七、实用选择流程

### 7.1 决策树

```
                    数据量 > 100万？
                    /            \
                  是              否
                  |               |
            序列长度>200？     序列长度<100？
            /        \         /        \
          是          否      是         否
          |           |       |          |
    Transformer    混合架构   LSTM      GRU/LSTM
```

### 7.2 快速选择表

| 你的情况 | 推荐架构 | 理由 |
|---------|---------|------|
| 刚入门，想快速验证 | **LSTM + PPO** | 成熟稳定，教程资源多 |
| 日频单标的策略 | **LSTM** | 简单有效 |
| 分钟级高频策略 | **GRU + DQN** | 低延迟 |
| 多资产组合优化 | **Transformer** | 捕捉资产间关系 |
| 技术形态识别 | **CNN** | 擅长局部模式 |
| 不确定，想稳妥 | **LSTM → 逐步复杂** | 避免过早优化 |

---

## 八、常见误区

**误区一：Transformer一定比LSTM好**

不对。在金融领域，数据量有限、信噪比低的情况下，LSTM常常更稳健。

**误区二：模型越复杂越好**

相反。金融数据噪音大，复杂模型容易过拟合。**简单模型 + 好的特征 > 复杂模型 + 差的特征**。

**误区三：照搬NLP/CV的架构配置**

金融数据有独特性质：非平稳、低信噪比、regime突变。需要针对性调整。

**误区四：只看回测指标选模型**

必须考虑：推理延迟、部署复杂度、可解释性需求。实盘中GRU可能比Transformer更实用。

---

## 九、技术选型建议总结

| 复杂度 | 数据关系 | 推荐架构 |
|-------|---------|---------|
| 简单线性关系 | 传统因子 | LightGBM/XGBoost |
| 中等复杂度 | 短期时序 | LSTM/GRU |
| 高度非线性 | 长期依赖 | Transformer |
| 需要动态决策 | 序列决策 | 强化学习（PPO） |
| 多模态数据 | 文本+数值 | LLM + LSTM混合 |

### 训练策略通用建议

1. **经验回放**：打破时序相关性，稳定训练
2. **目标网络**：延迟更新，减少震荡
3. **梯度裁剪**：防止梯度爆炸
4. **集成多模型**：降低单点风险
5. **严格历史验证**：Walk-Forward必不可少

---

## 十、延伸阅读

- [背景知识：强化学习在交易中的应用](强化学习在交易中的应用.md) - RL详细介绍
- [背景知识：前沿ML与RL方法（2025）](前沿ML与RL方法（2025）.md) - 最新技术进展
- [背景知识：时间序列交叉验证（Purged CV）](时间序列交叉验证（Purged%20CV）.md) - 正确的验证方法
- arXiv: "Vanilla LSTMs Outperform Transformer-based Forecasting"
- arXiv: "Higher Order Transformers: Enhancing Stock Movement Prediction"

---

> **核心认知**：模型架构选择不是追求最新最复杂，而是匹配你的数据规模、延迟要求和策略类型。从简单开始，逐步增加复杂度，用Walk-Forward验证每一步的决策。
