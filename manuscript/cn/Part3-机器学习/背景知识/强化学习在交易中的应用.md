# 背景知识：强化学习在交易中的应用

> "监督学习告诉你明天涨还是跌，强化学习告诉你该买多少、何时止损。"

---

## 为什么交易适合强化学习？

| 交易问题 | RL 框架映射 |
|---------|------------|
| 当前持仓、市场状态 | State（状态） |
| 买入/卖出/持有/仓位大小 | Action（动作） |
| 扣除成本后的收益 | Reward（奖励） |
| 交易成本、滑点 | Environment 反馈 |
| 最大化长期收益 | 目标函数 |

**关键洞察**：交易是**序列决策问题**，不是独立的预测问题。

---

## RL vs 监督学习：核心区别

| 维度 | 监督学习 | 强化学习 |
|-----|---------|---------|
| 目标 | 预测准确率 | 累计收益最大化 |
| 标签来源 | 历史数据（已知） | 行动结果（探索获得） |
| 决策连续性 | 每次独立预测 | 考虑后续影响 |
| 交易成本 | 事后扣除 | 内置于决策 |
| 仓位管理 | 额外逻辑 | 原生支持 |

**示例对比**：

```
监督学习思路：
1. 预测：明天 AAPL 涨 60% 概率
2. 行动：买入
3. 问题：买多少？已有持仓怎么办？

强化学习思路：
1. 状态：当前持仓 100 股，账户余额 $10,000，AAPL 在上涨趋势
2. 决策：根据 Q 值，应该加仓 50 股
3. 考虑：这个决策考虑了交易成本、仓位风险、和未来可能的走势
```

---

## 交易 RL 的核心要素

### 1. 状态空间（State）

| 状态类型 | 示例 | 维度 |
|---------|------|------|
| 价格特征 | 收益率、MA、RSI | 10-50 |
| 持仓信息 | 当前仓位、成本价、浮盈 | 3-5 |
| 账户信息 | 现金比例、已用保证金 | 2-3 |
| 市场状态 | 波动率、趋势强度 | 2-5 |

**状态设计原则**：
- 包含足够信息做决策
- 避免维度爆炸
- 归一化处理

### 2. 动作空间（Action）

**离散动作**（简单但粗糙）：
```
动作 = {强烈卖出, 卖出, 持有, 买入, 强烈买入}
映射 = {-100%, -50%, 0%, +50%, +100%}
```

**连续动作**（精细但复杂）：
```
动作 = 目标仓位比例 ∈ [-1, 1]
-1 = 做空 100%
 0 = 空仓
+1 = 做多 100%
```

### 3. 奖励函数（Reward）—— 最关键的设计

奖励函数设计是RL交易系统的**成败关键**。错误的奖励会导致Agent学习到完全相反的行为。

#### 四类奖励函数及适用场景

| 类型 | 公式 | 优点 | 缺点 | 适用场景 |
|-----|------|------|------|---------|
| **利润导向** | `r_t = position × return - cost` | 简单直观 | 忽略风险 | 入门/基准 |
| **风险调整** | `r_t = (return - rf) / volatility` | 考虑风险 | 忽略成本 | 中低频 |
| **成本惩罚** | `r_t = profit - α|Δpos| - β×spread` | 控制换手 | 参数敏感 | 高频 |
| **多目标复合** | `r_t = w1×profit + w2×sharpe - w3×drawdown` | 全面平衡 | 调参复杂 | 生产级 |

---

#### 类型一：利润导向（Profit-Oriented）

```python
# 最简单的奖励函数
r_t = position_{t-1} × (P_t - P_{t-1}) - transaction_cost

# 示例：
# 持仓 100 股，价格从 $100 涨到 $101，手续费 $5
# r_t = 100 × (101 - 100) - 5 = $95
```

**问题**：Agent会学习"全仓梭哈"策略——波动大时收益/亏损都大，不考虑风险。

---

#### 类型二：风险调整收益（Risk-Adjusted）

```python
# 类似即时夏普比率
r_t = (return_t - risk_free_rate) / volatility_t

# 其中：
# return_t = position × price_change / portfolio_value
# volatility_t = rolling_std(returns, window=20)

# 示例：
# 当日收益 0.5%，无风险利率 0.01%（日化），20日波动率 1%
# r_t = (0.5% - 0.01%) / 1% = 0.49
```

**优点**：鼓励追求风险调整后的收益，而非绝对收益。

**问题**：波动率低时奖励可能过大，需要平滑处理。

---

#### 类型三：交易成本惩罚（Cost-Penalized）

```python
# 专门针对高频策略，惩罚过度交易
r_t = profit_t - α × |position_t - position_{t-1}| - β × spread_t

# 参数说明：
# α = 换手惩罚系数（通常 0.001-0.01）
# β = 价差惩罚系数（通常 0.5-2.0）
# spread_t = bid-ask spread

# 示例：
# 盈利 $100，仓位变化 500 股，价差 $0.02/股
# α=0.005, β=1.0
# r_t = 100 - 0.005×500 - 1.0×0.02×500 = 100 - 2.5 - 10 = $87.5
```

**关键**：α 和 β 的选择需要反映真实交易成本，否则Agent行为会偏离实际。

---

#### 类型四：多目标复合奖励（Multi-Objective）

```python
# 生产级系统的推荐设计
r_t = w1 × profit_t + w2 × sharpe_t - w3 × drawdown_t - w4 × turnover_t

# 参数说明：
# w1 = 利润权重（通常 1.0）
# w2 = 夏普权重（通常 0.1-0.5）
# w3 = 回撤惩罚（通常 0.5-2.0）
# w4 = 换手惩罚（通常 0.01-0.1）

# 示例配置：
weights = {
    'profit': 1.0,       # 基础收益
    'sharpe': 0.3,       # 风险调整
    'drawdown': 1.5,     # 强惩罚回撤
    'turnover': 0.05     # 轻微换手惩罚
}
```

**调参建议**：
1. 先用利润导向训练baseline
2. 逐步加入风险惩罚项
3. 根据回测结果调整权重
4. 用验证集确定最终权重

---

#### 奖励函数设计的常见错误

| 错误 | 后果 | 解决方案 |
|-----|------|---------|
| 只用收益做奖励 | Agent全仓梭哈 | 加入风险惩罚项 |
| 忽略交易成本 | 高频但实盘亏损 | 加入换手惩罚 |
| 奖励scale过大 | 训练不稳定 | 归一化到 [-1, 1] |
| 奖励延迟太长 | 学习困难 | 使用中间奖励（浮盈变化）|
| 奖励过于复杂 | 调参困难 | 从简单开始迭代 |

---

#### 实用奖励函数代码

```python
import numpy as np

class TradingReward:
    """生产级奖励函数"""

    def __init__(self, config: dict):
        self.w_profit = config.get('profit_weight', 1.0)
        self.w_sharpe = config.get('sharpe_weight', 0.3)
        self.w_drawdown = config.get('drawdown_weight', 1.5)
        self.w_turnover = config.get('turnover_weight', 0.05)

    def calculate(self, state: dict) -> float:
        """计算复合奖励"""

        # 利润项
        profit = state['position'] * state['price_change']

        # 夏普项（即时版本）
        if state['volatility'] > 0:
            sharpe = state['return'] / state['volatility']
        else:
            sharpe = 0

        # 回撤惩罚
        drawdown = max(0, state['peak_value'] - state['current_value'])
        drawdown_pct = drawdown / state['peak_value'] if state['peak_value'] > 0 else 0

        # 换手成本
        turnover = abs(state['position'] - state['prev_position'])
        turnover_cost = turnover * state['transaction_cost']

        # 复合奖励
        reward = (
            self.w_profit * profit
            + self.w_sharpe * sharpe
            - self.w_drawdown * drawdown_pct
            - self.w_turnover * turnover_cost
        )

        # 归一化（可选）
        return np.clip(reward, -1.0, 1.0)
```

---

**简单奖励**（入门用）：
```
R_t = 仓位 × 收益率 - 交易成本
```

**风险调整奖励**：
```
R_t = 收益 - λ × 风险惩罚
    = 仓位 × 收益率 - λ × |收益率 - 均值|²
```

**夏普导向奖励**：
```
R_t = (收益率 - 无风险利率) / 波动率
```

---

## 常用 RL 算法及适用性

| 算法 | 类型 | 适用场景 | 优缺点 |
|-----|------|---------|-------|
| DQN | 离散动作，值函数 | 简单交易决策 | 易实现，动作受限 |
| DDPG | 连续动作，策略梯度 | 仓位优化 | 精细控制，训练难 |
| PPO | 通用，策略梯度 | 平衡选择 | 稳定，样本效率中等 |
| A2C/A3C | 并行训练 | 多标的同时训练 | 加速训练，需要资源 |
| SAC | 连续动作，最大熵 | 鼓励探索 | 防止早停，计算量大 |

**实践建议**：从 PPO 开始，它在稳定性和性能间平衡较好。

---

## 交易 RL 的特殊挑战

### 挑战一：样本效率低

```
对比：
- 游戏：无限生成训练样本
- 交易：历史数据有限

日线 10 年 ≈ 2,500 个样本
分钟线 5 年 ≈ 50 万个样本

解决方案：
- 使用更简单的模型
- 数据增强（加噪音、时间扭曲）
- 多标的并行训练
```

### 挑战二：非平稳环境

```
游戏规则固定，市场规则变化：
- 2019 年低波动环境
- 2020 年 COVID 崩盘
- 2021 年散户狂热
- 2022 年加息周期

解决方案：
- 滚动训练（每 N 天更新模型）
- Regime 条件训练
- 多环境训练
```

### 挑战三：奖励稀疏和延迟

```
问题：
- 今天买入，一周后才知道对错
- 短期亏损可能带来长期收益（加仓摊薄成本）

解决方案：
- 使用更高频的中间奖励
- 引入浮盈变化作为即时反馈
- 调整折扣因子 γ
```

### 挑战四：过拟合

```
症状：
- 训练集年化 100%+
- 测试集年化 -20%

解决方案：
- 简化模型结构
- 增加正则化
- 使用 Purged CV 验证
- 在不同市场周期测试
```

---

## 实际应用示例

### 单标的仓位管理

```
状态：
  - 过去 20 天收益率序列
  - 当前持仓比例
  - RSI, MACD 指标

动作：
  - 目标仓位 ∈ [0, 1]（只做多）

奖励：
  - 日收益率 × 仓位 - 换手成本

结果：
  - Agent 学会在趋势初期加仓
  - 在高波动期减仓
  - 自动控制换手率
```

### 多资产配置

```
状态：
  - N 个资产的特征向量
  - 当前配置权重
  - 组合波动率

动作：
  - 目标权重向量 ∈ [0, 1]^N，和为 1

奖励：
  - 组合夏普比率

结果：
  - Agent 学会分散投资
  - 动态调整风险敞口
  - 危机时减少高 Beta 资产
```

---

## 多智能体视角

RL 可以在多智能体架构中替代或增强特定 Agent：

```
方案一：RL 替代 Signal Agent
  - Signal Agent 原本用规则或监督模型
  - 改用 RL 直接输出交易信号
  - Risk Agent 仍用规则约束

方案二：RL 作为 Meta Agent
  - Signal Agent 提供多个信号
  - RL Meta Agent 决定如何组合
  - 学习不同信号在不同 Regime 的权重

方案三：多 RL Agent 协作
  - 每个资产一个 RL Agent
  - 全局 Risk Agent 协调仓位
  - 共享经验池加速学习
```

---

## 常见误区

**误区一：RL 可以完全替代人工规则**

不现实。RL 需要：
- 大量样本训练
- 稳定的奖励信号
- 合理的状态设计

这些都需要领域知识。RL 是增强工具，不是银弹。

**误区二：更复杂的 RL 算法效果更好**

在金融中往往相反。简单算法（DQN、PPO）加上好的状态设计，通常优于复杂算法（SAC、TD3）配粗糙设计。

**误区三：RL 训练好就可以直接上线**

危险。RL 可能学到：
- 过拟合历史数据的策略
- 在极端市场失效的行为
- 高换手无法实际执行的策略

必须经过严格的样本外测试和模拟交易。

---

## 实用建议

### 1. 从简单开始

```
起步配置：
- 算法：PPO
- 动作：离散 5 档
- 状态：< 20 维
- 单一标的
```

### 2. 奖励工程比模型重要

```
不好的奖励：只看收益
  → Agent 学会全仓梭哈

好的奖励：收益 - 风险惩罚 - 换手成本
  → Agent 学会风险管理
```

### 3. 验证流程

```
1. 训练集：60%
2. 验证集：20%（调超参）
3. 测试集：20%（最终评估，只用一次）

对比基线：
- 买入持有
- 简单动量策略
- 监督学习 + 规则
```

---

## 总结

| 要点 | 说明 |
|-----|------|
| RL 优势 | 端到端优化，自动考虑成本和仓位 |
| 核心挑战 | 样本少、环境变化、过拟合 |
| 推荐起点 | PPO + 离散动作 + 简单状态 |
| 成功关键 | 奖励设计 > 算法选择 > 模型结构 |
| 多智能体整合 | 可替代 Signal Agent 或作为 Meta Agent |
